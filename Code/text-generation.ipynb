{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Prompt results for GPT2 and LLaMA3**\n","\n","## Preparations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:33.528381Z","iopub.status.busy":"2024-09-02T08:24:33.527695Z","iopub.status.idle":"2024-09-02T08:24:43.498366Z","shell.execute_reply":"2024-09-02T08:24:43.497482Z","shell.execute_reply.started":"2024-09-02T08:24:33.528342Z"},"trusted":true},"outputs":[],"source":["# Import necessary packages\n","import os\n","import torch\n","import pandas as pd\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:43.503875Z","iopub.status.busy":"2024-09-02T08:24:43.503586Z","iopub.status.idle":"2024-09-02T08:24:43.544807Z","shell.execute_reply":"2024-09-02T08:24:43.543686Z","shell.execute_reply.started":"2024-09-02T08:24:43.503843Z"},"trusted":true},"outputs":[],"source":["# Check if a CUDA-enabled GPU is available for PyTorch to use\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## Define prompting function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:43.546831Z","iopub.status.busy":"2024-09-02T08:24:43.546274Z","iopub.status.idle":"2024-09-02T08:24:43.555641Z","shell.execute_reply":"2024-09-02T08:24:43.554673Z","shell.execute_reply.started":"2024-09-02T08:24:43.546794Z"},"trusted":true},"outputs":[],"source":["# Define Prompts\n","prompts = [\"Give a brief introduction about the Mardi Gras event in Sydney\",\n","            \"Answer in two sentences when and why the Mardi Gras event in Sydney emerged\",\n","            \"Provide the average annual attendance figures for the Sydney Mardi Gras event\", \n","            \"Assess whether Mardi Gras Sydney can improve its marketing\"]\n","# Define how many outputs to generate\n","num_return_sequences = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:43.559055Z","iopub.status.busy":"2024-09-02T08:24:43.558147Z","iopub.status.idle":"2024-09-02T08:24:43.569281Z","shell.execute_reply":"2024-09-02T08:24:43.568378Z","shell.execute_reply.started":"2024-09-02T08:24:43.559016Z"},"trusted":true},"outputs":[],"source":["def generate_evaluate_model(model, tokenizer, prompts, num_return_sequences, device):\n","    \"\"\"\n","    Generates and evaluates text using a pre-trained language model.\n","\n","    This function takes a list of prompts, generates text for each prompt using the specified model,\n","    and calculates the perplexity for both the input prompt and the generated text. The results are \n","    stored in a DataFrame for further analysis.\n","    \"\"\"\n","    # Define df for storing results\n","    df = pd.DataFrame()\n","    # Set model to evaluation mode\n","    model.eval()\n","    # Iterate over each prompt\n","    for prompt in prompts:\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","        \n","        outputs = model.generate(inputs[\"input_ids\"],\n","                                 attention_mask = inputs[\"attention_mask\"],\n","                                 do_sample=True,  # Enable sampling to generate diverse sequences.\n","                                 max_new_tokens = 512, # Maximum legth of context for GPT2\n","                                 top_p=0.95, # Use nucleus sampling (top-p)\n","                                 temperature = 1.0, # Use high remperature value\n","                                 num_return_sequences=num_return_sequences, # Define how many texts to generate\n","                                 # Set special tokens\n","                                 pad_token_id=tokenizer.eos_token_id, \n","                                 eos_token_id=tokenizer.eos_token_id \n","                             )\n","        \n","        # Iterate over each generated text\n","        for i in range(num_return_sequences):\n","            generated_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n","            # Remove the prompt from the output\n","            generated_text = generated_text.replace(prompt,'').strip()\n","            generated_text_tokenized = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n","            \n","            # Create a DataFrame storing results for one generated text\n","            data = {\n","                \"prompt\": [prompt], # Save prompt\n","                \"output\": [generated_text], # Sabe prompt output\n","            }\n","            output_df = pd.DataFrame(data)\n","            # Add row to df by concatenation\n","            df = pd.concat([df,output_df])\n","    \n","    return df\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## GPT 2"]},{"cell_type":"markdown","metadata":{},"source":["### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:43.571317Z","iopub.status.busy":"2024-09-02T08:24:43.570609Z","iopub.status.idle":"2024-09-02T08:24:44.691501Z","shell.execute_reply":"2024-09-02T08:24:44.690341Z","shell.execute_reply.started":"2024-09-02T08:24:43.571271Z"},"trusted":true},"outputs":[],"source":["# Load the model directly from Huggingface\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:07:38.851147Z","iopub.status.busy":"2024-09-02T08:07:38.850748Z","iopub.status.idle":"2024-09-02T08:07:59.982010Z","shell.execute_reply":"2024-09-02T08:07:59.980984Z","shell.execute_reply.started":"2024-09-02T08:07:38.851104Z"},"trusted":true},"outputs":[],"source":["df = generate_evaluate_model(model, tokenizer, prompts, num_return_sequences, device)\n","# Save to Excel\n","df.to_excel(\"gpt2_rating.xlsx\", index = False)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## LLaMA3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T08:24:45.263703Z","iopub.status.busy":"2024-09-02T08:24:45.263296Z","iopub.status.idle":"2024-09-02T08:25:07.121584Z","shell.execute_reply":"2024-09-02T08:25:07.120490Z","shell.execute_reply.started":"2024-09-02T08:24:45.263665Z"},"trusted":true},"outputs":[],"source":["# Set HF access token to use LLaMA3\n","os.environ['HF_TOKEN']=\"your_token\"\n","os.environ['HUGGINGFACEHUB_API_TOKEN']= \"your_token\"\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n","# Load model\n","pipeline = pipeline(\n","    \"text-generation\",\n","    model=\"meta-llama/Meta-Llama-3-8B\",\n","    tokenizer=tokenizer,\n","    model_kwargs={\"torch_dtype\":torch.float16},\n","    device_map=device\n",")\n","model = pipeline.model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-02T08:09:55.898153Z","iopub.status.idle":"2024-09-02T08:09:55.898612Z","shell.execute_reply":"2024-09-02T08:09:55.898433Z","shell.execute_reply.started":"2024-09-02T08:09:55.898406Z"},"trusted":true},"outputs":[],"source":["df = generate_evaluate_model(model, tokenizer, prompts, num_return_sequences, device)\n","# Save to Excel\n","df.to_excel(\"llama3_results.xlsx\", index=False)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5563362,"sourceId":9201773,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
